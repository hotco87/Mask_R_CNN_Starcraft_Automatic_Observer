{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9189a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8d3d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PennFudanPed/PNGImages/FudanPed00001.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_14504/2294685092.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PennFudanPed/PNGImages/FudanPed00001.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\starcraft\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2974\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2975\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PennFudanPed/PNGImages/FudanPed00001.png'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "kk = Image.open('PennFudanPed/PNGImages/FudanPed00001.png')\n",
    "np.shape(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e843d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e639027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['boxes', 'labels', 'masks', 'image_id', 'area', 'iscrowd'])\n",
      "tensor([419., 170., 534., 485.])\n",
      "tensor([1, 1])\n",
      "tensor([0])\n",
      "tensor([35358., 36225.])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "dataset = PennFudanDataset('PennFudanPed/')\n",
    "dataset[0]\n",
    "import matplotlib.pyplot as plt\n",
    "# print(dataset[0][0]) ### <- image\n",
    "plt.imshow(dataset[0][1]['masks'][0])\n",
    "plt.imshow(dataset[0][1]['masks'][1])\n",
    "plt.show()\n",
    "print(dataset[0][1].keys()) ### <- image\n",
    "print(dataset[0][1]['boxes'][1])\n",
    "print(dataset[0][1]['labels'])\n",
    "print(dataset[0][1]['image_id'])\n",
    "print(dataset[0][1]['area'])\n",
    "print(dataset[0][1]['iscrowd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb487c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][1]['boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0][1]['masks'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[0][1]['masks'][0].mul(255).byte().detach().numpy())\n",
    "# Image.fromarray(dataset[0][1]['masks'][0].mul(255).byte().numpy())\n",
    "# Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433348b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]['masks'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fde56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c684a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ffa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[42][1]['masks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[42][1]['boxes'][1])\n",
    "print(dataset[42][1]['labels'])\n",
    "print(dataset[42][1]['image_id'])\n",
    "print(dataset[42][1]['area'])\n",
    "print(dataset[42][1]['iscrowd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70cfd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios \n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082653a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55554a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f2a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions1 = model(x)           # Returns predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed56066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5994f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6083f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fded92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118863f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchsummary import summary\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "# from torchsummary import summary as summary_\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)#.features\n",
    "    # print(model.in_channels)\n",
    "    # print(model.out_channels)\n",
    "    model.backbone.body.conv1 = nn.Conv2d(11, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    print(\"in_features\", in_features) #\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    print(\"in_features_mask:\", in_features_mask)\n",
    "    print(\"hidden_layer:\", hidden_layer)\n",
    "    print(\"num_classes:\", num_classes)\n",
    "    # in_features_mask: 256\n",
    "    # hidden_layer: 256\n",
    "    # num_classes: 2\n",
    "\n",
    "    return model\n",
    "\n",
    "num_classes = 2\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "# print(model)\n",
    "model.eval()\n",
    "x = [torch.rand(11, 128, 128), torch.rand(1, 128, 128)]\n",
    "x = [torch.rand(11, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)\n",
    "print(np.shape(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5372ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d375dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['masks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48de6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71041ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(predictions[0]['masks'][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['masks'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['masks'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfca28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predictions[0]['masks'][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bbd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
